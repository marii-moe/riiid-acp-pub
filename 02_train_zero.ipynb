{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run distributed:\n",
    "\n",
    "```\n",
    "make\n",
    "```\n",
    "\n",
    "```\n",
    "CUDA_VISIBLE_DEVICES=3,4,5,1,2,0 python -m torch.distributed.launch --master_port 1235 --nproc_per_node=6 02_train.py --epochs 30 --bs 96 --fp16 to_fp16 --trf_heads 4 --mixup False --chunk_size 500 --trf_dim 512 --loss ce --n_chunks 1 --fit fit_flat_cos --fit_kwargs pct_start=0.5 div_final=100 --tfixup True --pad r --valid_pct 0.025 --trf_act gelu --opt ranger_lamb --lr 3e-3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.basics       import *\n",
    "from fastai.callback.all import *\n",
    "from fastai.distributed  import *\n",
    "from fastai.tabular.all  import *\n",
    "from fastai.test_utils   import *\n",
    "\n",
    "import ast\n",
    "import enum\n",
    "import gc\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import enum\n",
    "\n",
    "from collections import defaultdict\n",
    "from fastcore.script import *\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "from pytorch_block_sparse.util import ModelPatcher\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.distributions.beta import Beta\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributed.optim.zero_redundancy_optimizer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(rank_distrib()==0):\n",
    "    from fastai.callback.wandb import WandbCallback\n",
    "    import wandb\n",
    "    wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_d = Path('../input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#13) [Path('../input/zip.zip'),Path('../input/example_sample_submission.csv'),Path('../input/example_test.csv'),Path('../input/lectures.csv'),Path('../input/questions.csv'),Path('../input/riiideducation'),Path('../input/train.csv'),Path('../input/data_v210101b.pkl'),Path('../input/meta_v210101b.pkl'),Path('../input/data_v210101b_sample.pkl')...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_d.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@call_parse\n",
    "def main(\n",
    "    model:         Param(\"Name\", str) = '210105',\n",
    "    data:          Param(\"Data version\", str) = '210101b',\n",
    "    load:          Param(\"Load from\", str) = None,\n",
    "    validate:      Param(\"\", action='store_true') = False,\n",
    "    chunk_size:    Param(\"Chunk size\", int) = 500,\n",
    "    n_chunks:      Param(\"Number of chunks\", int) = 1,\n",
    "    #chunk_size:    Param(\"Chunk size\", int) = 50,  \n",
    "    #n_chunks:      Param(\"Number of chunks\", int) = 10,    \n",
    "    #bs:            Param(\"BS\", int) = 96,\n",
    "    bs:            Param(\"BS\", int) = 48,    #2 works, 96 doesn't\n",
    "    workers:       Param(\"\", int) = 8,\n",
    "    valid_pct:     Param(\"Validation set\", float) = 0.025,\n",
    "    trf_dim:       Param(\"\", int) = 512,\n",
    "    trf_enc:       Param(\"\", int) = 4,\n",
    "    trf_dec:       Param(\"\", int) = 4,\n",
    "    trf_heads:     Param(\"\", int) = 4,\n",
    "    trf_do:        Param(\"\", float) = 0.1,\n",
    "    trf_act:       Param(\"\", str) = 'gelu',\n",
    "    lr:            Param(\"\", float) = 3e-3,\n",
    "    clip:          Param(\"\", float) = 0.,\n",
    "    \n",
    "    moms:          Param(\"Moms for fit_one_cycle\", float, nargs='+') = (0.95,0.85,0.95),\n",
    "    epochs:        Param(\"Epochs\", int) = 5, #Original was 30,\n",
    "    tfixup:        Param(\"Use T-Fixup init\", ast.literal_eval) = True,\n",
    "    mixup:         Param(\"Use mixup\", ast.literal_eval) = False,\n",
    "    opt:           Param(\"Optimizer\", str) = 'ranger_lamb',\n",
    "    zero_stage:    Param(\"Zero Stage(0 is off)\", int) = 0,\n",
    "    opt_kwargs:    Param(\"Optional args for opt, eg. eps=1e-4\", str, nargs='+') = {},\n",
    "    fit:           Param(\"fit or fit_one_cycle\", str) = 'fit_flat_cos',\n",
    "    fit_kwargs:    Param(\"Optional args for fit,eg pct_start=0.1\", str, nargs='+') = ['pct_start=0.5', 'div_final=100.'],\n",
    "    fp16:          Param(\"fp16 method: to_fp16, to_native_fp16, none\", str) = 'to_fp16',\n",
    "    \n",
    "    loss:          Param(\"Loss\", str) = 'ce',\n",
    "    \n",
    "    wua:           Param(\"Weight of user_answer term in the loss\", float) = 0.,\n",
    "    pad:           Param (\"Pad left of right (l|r)\",str,choices=['l','r'])='r',\n",
    "\n",
    "    local_rank:    Param(\"--local_rank\", int) = None,\n",
    "): \n",
    "    if opt_kwargs: opt_kwargs = {s.split('=')[0]:float(s.split('=')[1]) for s in opt_kwargs}\n",
    "    if fit_kwargs: fit_kwargs = {s.split('=')[0]:float(s.split('=')[1]) for s in fit_kwargs}\n",
    "    print(locals())\n",
    "    globals().update({ 'H' : AttrDict(locals())})\n",
    "_H = AttrDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': '210105', 'data': '210101b', 'load': None, 'validate': False, 'chunk_size': 500, 'n_chunks': 1, 'bs': 48, 'workers': 8, 'valid_pct': 0.025, 'trf_dim': 512, 'trf_enc': 4, 'trf_dec': 4, 'trf_heads': 4, 'trf_do': 0.1, 'trf_act': 'gelu', 'lr': 0.003, 'clip': 0.0, 'moms': (0.95, 0.85, 0.95), 'epochs': 5, 'tfixup': True, 'mixup': False, 'opt': 'ranger_lamb', 'zero_stage': 0, 'opt_kwargs': {}, 'fit': 'fit_flat_cos', 'fit_kwargs': {'pct_start': 0.5, 'div_final': 100.0}, 'fp16': 'to_fp16', 'loss': 'ce', 'wua': 0.0, 'pad': 'r', 'local_rank': None}\n"
     ]
    }
   ],
   "source": [
    "#noexport\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noexport\n",
    "H.bs=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpus = torch.cuda.device_count() if H.local_rank is None else 1\n",
    "if H.local_rank is not None:\n",
    "    torch.cuda.set_device(H.local_rank)\n",
    "    torch.distributed.init_process_group(backend='nccl', init_method='env://')\n",
    "    print(f\"DISTRIBUTED: {H.local_rank}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read df and meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO detect if meta exists and don't load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(in_d / f'meta_v{H.data}_sample.pkl', 'rb') as f:\n",
    "    meta = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "QCols = enum.IntEnum('QCols', meta.qcols, start=0)\n",
    "LCols = enum.IntEnum('LCols', meta.lcols, start=0)\n",
    "Cats  = enum.IntEnum('Cats',  meta.cat_names, start=0)\n",
    "Conts = enum.IntEnum('Conts', meta.cont_names, start=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "with open(in_d / f'data_v{H.data}_sample.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46, 11)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.cat_d[115].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO detect if data exists and don't load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del data.attempt_num_coo\n",
    "del data.attempts_correct_coo\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lut = meta.icats['answered_correctly'],meta.icats['user_answer']\n",
    "y_d = {}\n",
    "for k, v in data.cat_d.items():\n",
    "    y_d[k] = np.column_stack((lut[0][v[:,Cats.answered_correctly] - 1],lut[1][v[:,Cats.user_answer] - 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chop sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chop_sequence(d):\n",
    "    nv = defaultdict(dict)\n",
    "    for k, v in d.items():\n",
    "        i = 0\n",
    "        while i*H.chunk_size < len(v):\n",
    "            nv[k][i] = v[i*H.chunk_size:(i+1)*H.chunk_size]\n",
    "            i += 1\n",
    "    return nv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_d  = chop_sequence(data.cat_d)\n",
    "cont_d = chop_sequence(data.cont_d)\n",
    "tags_d = chop_sequence(data.tags_d)\n",
    "tagw_d = chop_sequence(data.tagw_d)\n",
    "y_d    = chop_sequence(y_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert np.concatenate(list(cat_d.values())).shape[0] == np.concatenate(list(data.cat_d.values())).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 78011 different users\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(data.cat_d)} different users')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_keys = sorted(list(cat_d.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last H.valid_pct is valid set\n",
    "train_group_keys = group_keys[:int((1 - H.valid_pct) * len(group_keys))]\n",
    "valid_group_keys = group_keys[int((1 - H.valid_pct) * len(group_keys)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users: train=76060, valid=1951\n"
     ]
    }
   ],
   "source": [
    "print(f'users: train={len(train_group_keys)}, valid={len(valid_group_keys)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dict(d, keys):\n",
    "    return { (u, t): d[u][t] for u in keys for t in d[u].keys() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_cat =  split_dict(cat_d, train_group_keys)\n",
    "train_x_cont = split_dict(cont_d, train_group_keys)\n",
    "train_x_tags = split_dict(tags_d, train_group_keys)\n",
    "train_x_tagw = split_dict(tagw_d, train_group_keys)\n",
    "train_y =      split_dict(y_d, train_group_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x_cat =  split_dict(cat_d, valid_group_keys)\n",
    "valid_x_cont = split_dict(cont_d, valid_group_keys)\n",
    "valid_x_tags = split_dict(tags_d, valid_group_keys)\n",
    "valid_x_tagw = split_dict(tagw_d, valid_group_keys)\n",
    "valid_y =      split_dict(y_d, valid_group_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seqs: train=100105, valid=2717\n"
     ]
    }
   ],
   "source": [
    "print(f'seqs: train={len(train_x_cat)}, valid={len(valid_x_cat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionsDataset(Dataset):\n",
    "    def __init__(self, x_cat, x_cont, x_tags, x_tagw, y, minids=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.means = np.expand_dims(meta.means, axis=0) # ready to broadcast\n",
    "        self.stds  = np.expand_dims(meta.stds , axis=0)\n",
    "        \n",
    "        self.n_inp = 5  # number of feature (x) tensors\n",
    "        \n",
    "        self.x_cat = x_cat  # SL, XF (sequence len, feature columns) \n",
    "        self.x_cont = x_cont\n",
    "        self.x_tags = x_tags      \n",
    "        self.x_tagw = x_tagw\n",
    "        self.y = y  # SL, 1\n",
    "        \n",
    "        self.keys = list(self.x_cat.keys()) # list of group keys\n",
    "        \n",
    "        if minids:\n",
    "            self.keys = self.keys[:H.bs*2]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys) # H.bs * 2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user_id, time_slice = self.keys[idx]\n",
    "        win = range(max(0, time_slice - H.n_chunks + 1), time_slice + 1)\n",
    "        x_cat  = np.concatenate([ self.x_cat [(user_id, ts)] for ts in win ])\n",
    "        x_cont = np.concatenate([ self.x_cont[(user_id, ts)] for ts in win ])\n",
    "        x_tags = np.concatenate([ self.x_tags[(user_id, ts)] for ts in win ])\n",
    "        x_tagw = np.concatenate([ self.x_tagw[(user_id, ts)] for ts in win ])\n",
    "        y      = np.concatenate([ self.y     [(user_id, ts)] for ts in win ])\n",
    "        \n",
    "        pad = H.chunk_size * H.n_chunks - x_cat.shape[0]\n",
    "        \n",
    "        # Normalize x_cont\n",
    "        x_cont = (x_cont - self.means) / self.stds\n",
    "        x_cont[np.isnan(x_cont)] = 0\n",
    "        \n",
    "        padt = (0,pad) if H.pad == 'r' else (pad,0)\n",
    "        \n",
    "        x_mask = np.zeros(x_cat.shape[0], dtype=np.bool)\n",
    "        \n",
    "        x_mask = np.pad(x_mask, padt, constant_values=(True))\n",
    "        x_cat  = np.pad(x_cat , (padt, (0, 0)), constant_values=(0)).astype(np.int64)\n",
    "        x_cont = np.pad(x_cont, (padt, (0, 0)), constant_values=(0)).astype(np.float32)\n",
    "        x_tags = np.pad(x_tags, (padt, (0, 0)), constant_values=(0)).astype(np.int64)\n",
    "        x_tagw = np.pad(x_tagw, (padt, (0, 0)), constant_values=(0.)).astype(np.float32)\n",
    "        y      = np.pad(y,      (padt, (0, 0)), constant_values=(-1)).astype(np.int64)\n",
    "\n",
    "        return x_mask, x_cat, x_cont, x_tags, x_tagw, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = InteractionsDataset(train_x_cat, train_x_cont, train_x_tags, train_x_tagw, train_y)\n",
    "valid_ds = InteractionsDataset(valid_x_cat, valid_x_cont, valid_x_tags, valid_x_tagw, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21607/3307222027.py:41: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  x_mask = np.zeros(x_cat.shape[0], dtype=np.bool)\n"
     ]
    }
   ],
   "source": [
    "x_mask, x_cat, x_cont, x_tags, x_tagw, y = train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100105"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds.keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_tagw[-47:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert x_cat.shape == (H.chunk_size*H.n_chunks, len(meta.cat_names))\n",
    "assert x_cont.shape == (H.chunk_size*H.n_chunks, len(meta.cont_names))\n",
    "assert x_tags.shape == x_tagw.shape == (H.chunk_size*H.n_chunks, 6)\n",
    "assert y.shape == (H.chunk_size*H.n_chunks, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, bs=H.bs, shuffle=True, drop_last=True, num_workers=H.workers)\n",
    "valid_dl = DataLoader(valid_ds, bs=H.bs,                               num_workers=H.workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21607/3307222027.py:41: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  x_mask = np.zeros(x_cat.shape[0], dtype=np.bool)\n"
     ]
    }
   ],
   "source": [
    "x_mask,x_cat, x_cont, x_tags, x_tagw, y = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4, suppress=True)\n",
    "torch.set_printoptions(precision=4, linewidth=200, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3068, -0.3605, -0.1892,  ..., -0.0742,  0.0000,  0.0000],\n",
       "        [-0.3068, -0.3605, -0.1892,  ..., -0.0739, -0.0424, -0.5713],\n",
       "        [-0.3068, -0.3605, -0.1892,  ..., -0.0735, -0.0424, -0.5848],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cont[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 500, 11]),\n",
       " torch.Size([16, 500, 23]),\n",
       " torch.Size([16, 500, 6]),\n",
       " torch.Size([16, 500, 6]),\n",
       " torch.Size([16, 500, 2]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cat.shape, x_cont.shape, x_tags.shape, x_tagw.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert x_cat.isnan().any() == False\n",
    "assert x_cont.isnan().any() == False\n",
    "assert x_tags.isnan().any() == False\n",
    "assert x_tagw.isnan().any() == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert x_cat.shape == (H.bs, H.chunk_size*H.n_chunks, len(meta.cat_names))\n",
    "assert x_cont.shape == (H.bs, H.chunk_size*H.n_chunks, len(meta.cont_names))\n",
    "assert x_tags.shape == x_tagw.shape == (H.bs, H.chunk_size*H.n_chunks, 6)\n",
    "assert y.shape == (H.bs, H.chunk_size*H.n_chunks, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc(pred, targ):\n",
    "    pred = torch.softmax(pred, dim=2)\n",
    "    pred = pred[:,:,1:2] # prediction for True\n",
    "    idx = targ != -1\n",
    "    pred = pred[idx]\n",
    "    targ = targ[idx]\n",
    "    pred, targ = flatten_check(pred, targ)\n",
    "    if len(targ.unique()) == 2:\n",
    "        return roc_auc_score(targ.cpu().numpy(), pred.cpu().numpy())\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss if H.loss=='ce' else globals()[H.loss]\n",
    "loss    = loss_fn(ignore_index=-1)\n",
    "loss_nr = loss_fn(ignore_index=-1, reduction='none')\n",
    "\n",
    "def loss_func(pred, targ, shuffle=None, lam=None):\n",
    "    b, s, l = pred.shape\n",
    "    if shuffle is not None:\n",
    "        targ_shuffled = targ[shuffle].view(b*s)\n",
    "    pred = pred.view(b*s, l)\n",
    "    targ = targ.view(b*s)\n",
    "\n",
    "    if shuffle is not None:\n",
    "        l0 = loss_nr(pred, targ).view(b, s)\n",
    "        l1 = loss_nr(pred, targ_shuffled).view(b, s)\n",
    "        return torch.lerp(l0, l1, lam.view(lam.shape[0], 1)).mean()\n",
    "    else:\n",
    "        #print(targ.unique()) # CUDA assert error if any index here is bigger than dimension l (labels) of pred\n",
    "        return loss(pred, targ)\n",
    "    \n",
    "def ua_loss_func(pred, targ, shuffle=None, lam=None):\n",
    "    loss_fn = loss_func\n",
    "    l = loss_fn(pred[...,:2],targ[...,:1],shuffle,lam) \n",
    "    if H.wua and targ.shape[-1]>1: l += H.wua * loss_fn(pred[...,2:],targ[...,1:],shuffle,lam)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noexport\n",
    "_p = torch.zeros([32, 127, 6])\n",
    "_t = torch.empty ([32, 127,2]).type(torch.long)\n",
    "_t[...,0] = torch.randint(2,_t.shape[:2])\n",
    "_t[...,1] = torch.randint(4,_t.shape[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#noexport\n",
    "roc_auc(_p[...,:2], _t[...,:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6931)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#noexport\n",
    "loss_func(_p[...,:2], _t[...,:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6931)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#noexport\n",
    "ua_loss_func(_p, _t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LBMetric(Metric):\n",
    "    def __init__(self, loss_func, name):\n",
    "        self.loss_func = loss_func\n",
    "        self.nam = name\n",
    "        \n",
    "    def reset(self):\n",
    "        self.targs, self.preds = [], []\n",
    "        \n",
    "    def accumulate(self, learn):\n",
    "        self.preds.append(learn.to_detach(learn.pred[...,:2]))\n",
    "        self.targs.append(learn.to_detach(learn.y[...,:1]))\n",
    "    \n",
    "    @property\n",
    "    def value(self):\n",
    "        if len(self.preds) == 0: return\n",
    "        preds = torch.cat(self.preds)\n",
    "        targs = torch.cat(self.targs)\n",
    "        r = self.loss_func(preds, targs)\n",
    "        return r\n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.nam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#noexport\n",
    "lam = Beta(0.5, 0.5).sample((10000,))\n",
    "lam = torch.stack([lam, 1-lam], 1)\n",
    "lam = lam.max(1)[0].numpy()\n",
    "_ = plt.hist(lam, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMixUp(Callback):\n",
    "    run_after,run_valid = [Normalize],False\n",
    "    def __init__(self, alpha=0.4): \n",
    "        self.distrib = Beta(tensor(alpha), tensor(alpha))\n",
    "\n",
    "    def before_batch(self):\n",
    "        lam = self.distrib.sample((self.y.size(0),)).squeeze().to(self.y.device)\n",
    "        lam = torch.stack([lam, 1-lam], 1)\n",
    "        lam = lam.max(1)[0]\n",
    "        shuffle = torch.randperm(self.y.size(0)).to(self.y.device)\n",
    "        self.learn.xb = (*self.xb, shuffle, lam)\n",
    "        self.learn.yb = (*self.yb, shuffle, lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientClipping(Callback):\n",
    "    \"Gradient clipping during training.\"\n",
    "    def __init__(self, clip:float = 0.):\n",
    "        self.clip = clip\n",
    "\n",
    "    def after_backward(self, **kwargs):\n",
    "        \"Clip the gradient before the optimizer step.\"\n",
    "        if self.clip: nn.utils.clip_grad_norm_(self.learn.model.parameters(), self.clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TutorNet(nn.Module):\n",
    "    def __init__(self, emb_szs, tag_emb_szs, emb_do, n_cont, trf_dim, trf_enc, trf_dec, trf_heads, trf_do, trf_act):\n",
    "        super().__init__()\n",
    "        self.nhead,self.trf_dim = trf_heads, trf_dim\n",
    "        \n",
    "        tag_emb_szs =(tag_emb_szs[0]+1, trf_dim)\n",
    "\n",
    "        self.embeds    = nn.ModuleList([nn.Sequential(nn.Embedding(ni+1, nf, max_norm=1.),nn.Linear(nf, trf_dim)) \n",
    "                                        for ni, nf in emb_szs])\n",
    "        self.tagembeds = nn.EmbeddingBag(*tag_emb_szs, max_norm=1., mode='sum')\n",
    "        self.conts     = nn.Linear(n_cont, trf_dim)\n",
    "            \n",
    "        self.trafo = nn.Transformer(\n",
    "            d_model = trf_dim,\n",
    "            nhead = trf_heads,\n",
    "            num_encoder_layers = trf_enc,\n",
    "            num_decoder_layers = trf_dec,\n",
    "            dim_feedforward = trf_dim*4,\n",
    "            dropout = trf_do,\n",
    "            activation = trf_act,\n",
    "        )\n",
    "\n",
    "        self.mlp = nn.Linear(trf_dim, 6)\n",
    "        \n",
    "    def forward(self, x_mask, x_cat, x_cont, x_tags, x_tagw, shuffle=None, lam=None):\n",
    "        b, sl, catf, contf, tagsf = (*x_cat.shape, x_cont.shape[2], x_tags.shape[2])\n",
    "        \n",
    "        x_cat  += 1\n",
    "        x_tags += 1\n",
    "    \n",
    "        # compute masks\n",
    "        causal_mask  = torch.triu(torch.ones(1,sl, sl,dtype=torch.bool,device=x_cat.device), diagonal=1).expand(b,-1,-1)\n",
    "        x_tci   = x_cat[...,Cats.task_container_id]\n",
    "        x_tci_s = torch.zeros_like(x_tci)\n",
    "        x_tci_s[...,1:] = x_tci[...,:-1]\n",
    "        enc_container_aware_mask =  (x_tci.unsqueeze(-1) == x_tci_s.unsqueeze(-1).permute(0,2,1)) | causal_mask\n",
    "        dec_container_aware_mask = ~(x_tci.unsqueeze(-1) == x_tci.unsqueeze(-1).permute(0,2,1))   & causal_mask\n",
    "\n",
    "        padding_mask = x_mask \n",
    "                \n",
    "        # encoder x (shifted q & a)\n",
    "        enc_cat  = torch.zeros_like(x_cat)\n",
    "        enc_cont = torch.zeros_like(x_cont)\n",
    "        enc_tags = torch.zeros_like(x_tags)\n",
    "        enc_tagw = torch.zeros_like(x_tagw)\n",
    "        \n",
    "        enc_cat[:,1:]  = x_cat[:,:-1]\n",
    "        enc_cont[:,1:] = x_cont[:,:-1]\n",
    "        enc_tags[:,1:] = x_tags[:,:-1]\n",
    "        enc_tagw[:,1:] = x_tagw[:,:-1]\n",
    "        \n",
    "        # decoder x (nonshifted q)\n",
    "        dec_cat  = x_cat\n",
    "        dec_cont = x_cont\n",
    "        dec_tags = x_tags\n",
    "        dec_tagw = x_tagw\n",
    "\n",
    "        # hide correct answer and user answered correctly from decoder\n",
    "        dec_cat[...,Cats.answered_correctly] = 0\n",
    "        dec_cat[...,Cats.user_answer] = 0\n",
    "        dec_cat[...,Cats.qhe] = 0\n",
    "        dec_cont[...,Conts.qet] = 0\n",
    "        dec_cont[...,Conts.qet_log] = 0\n",
    "        \n",
    "        # print(enc_cont.shape)\n",
    "        enc_cat  =  enc_cat.view(b * sl, catf)   # b*sl, catf\n",
    "        enc_tags = enc_tags.view(b * sl, tagsf) # b*sl, tagsf\n",
    "        enc_tagw = enc_tagw.view(b * sl, tagsf) # b*sl, tagsf\n",
    "\n",
    "        dec_cat  =  dec_cat.view(b * sl, catf)   # b*sl, catf\n",
    "        dec_tags = dec_tags.view(b * sl, tagsf) # b*sl, tagsf\n",
    "        dec_tagw = dec_tagw.view(b * sl, tagsf) # b*sl, tagsf\n",
    "        \n",
    "        # embed categorical vars\n",
    "        enc = torch.mean(torch.stack([\n",
    "            *[ e(enc_cat[:,i]) for i, e in enumerate(self.embeds) ],\n",
    "            self.tagembeds(enc_tags, per_sample_weights=enc_tagw),\n",
    "            self.conts(enc_cont).view(-1,self.trf_dim)\n",
    "        ]),dim=0)\n",
    "        \n",
    "        dec = torch.mean(torch.stack([\n",
    "            *[ e(dec_cat[:,i]) for i, e in enumerate(self.embeds) ],\n",
    "            self.tagembeds(dec_tags, per_sample_weights=dec_tagw),\n",
    "            self.conts(dec_cont).view(-1,self.trf_dim)\n",
    "        ]),dim=0)\n",
    "        \n",
    "        enc = enc.view(b, sl, self.trf_dim)           # b, sl, sum of cat, cont and tag ftrs\n",
    "        dec = dec.view(b, sl, self.trf_dim)           # b, sl, sum of cat, cont and tag ftrs\n",
    "\n",
    "        if shuffle is not None:\n",
    "            enc = torch.lerp(enc, enc[shuffle], lam.view(lam.shape[0], 1, 1))\n",
    "            dec = torch.lerp(dec, dec[shuffle], lam.view(lam.shape[0], 1, 1))\n",
    "            padding_mask = None\n",
    "            enc_container_aware_mask = dec_container_aware_mask = causal_mask | causal_mask[shuffle]\n",
    "        \n",
    "        enc = enc.permute(1, 0, 2)          # sl, b, tf (torchformer input)\n",
    "        dec = dec.permute(1, 0, 2)          # sl, b, tf\n",
    "\n",
    "        expand_nheads = lambda t: t.unsqueeze(1).expand(t.shape[0],self.nhead,-1,-1).reshape(-1,*t.shape[-2:])\n",
    "        \n",
    "        o = self.trafo(\n",
    "            enc, \n",
    "            dec, \n",
    "            src_mask = expand_nheads(enc_container_aware_mask),\n",
    "            tgt_mask = expand_nheads(dec_container_aware_mask),\n",
    "            memory_mask = expand_nheads(enc_container_aware_mask),\n",
    "            src_key_padding_mask = padding_mask,\n",
    "            tgt_key_padding_mask = padding_mask,\n",
    "            memory_key_padding_mask = padding_mask,\n",
    "        )                                   # sl, b, tf\n",
    "        o = o.permute(1, 0, 2)              # b, sl, tf\n",
    "        o = self.mlp(o)                     # b, sl, of (of=2)\n",
    "        #print(o)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_szs = list(zip(meta.n_emb.values(), meta.emb_dim.values()))\n",
    "tag_emb_szs = meta.tags_n_emb, meta.tags_emb_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TutorNet(emb_szs, tag_emb_szs, None, len(meta.cont_names), \n",
    "                 H.trf_dim, H.trf_enc, H.trf_dec, H.trf_heads, H.trf_do, H.trf_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39959744"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([p.numel() for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-Fixup init\n",
    "\n",
    "1. Apply Xavier initialization for all parameters excluding input embeddings. Use Gaussian initialization $N(0,d^{-\\frac{1}{2}})$ for input embeddings where d is the embedding dimension.\n",
    "\n",
    "2. Scale $v_{d}$ and $w_{d}$ matrices in each decoder attention block, weight matrices in each decoder MLP block and input embeddings $x$ and $y$ in encoder and decoder by $(9N)^{−\\frac{1}{4}}$: [code](https://github.com/layer6ai-labs/T-Fixup/blob/f1fae213ce7b48829f81632d0c96bb039b7c450e/fairseq/modules/transformer_layer.py#L161), [code](https://github.com/layer6ai-labs/T-Fixup/blob/f1fae213ce7b48829f81632d0c96bb039b7c450e/fairseq/models/transformer.py#L378), [code](https://github.com/layer6ai-labs/T-Fixup/blob/f1fae213ce7b48829f81632d0c96bb039b7c450e/fairseq/models/transformer.py#L604)\n",
    "\n",
    "3. Scale $v_{e}$ and $w_{e}$ matrices in each encoder attention block and weight matrices in each encoder MLP block by $0.67N^{−\\frac{1}{4}}$: [code](https://github.com/layer6ai-labs/T-Fixup/blob/f1fae213ce7b48829f81632d0c96bb039b7c450e/fairseq/modules/transformer_layer.py#L36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trunc_normal_(x, mean=0., std=1.):\n",
    "    \"Truncated normal initialization (approximation)\"\n",
    "    # From https://discuss.pytorch.org/t/implementing-truncated-normal-initializer/4778/12\n",
    "    return x.normal_().fmod_(2).mul_(std).add_(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "if H.tfixup:\n",
    "    for n,p in model.named_parameters():\n",
    "        if re.match(r'.*bias$|.*bn\\.weight$|.*norm.*\\.weight',n): continue\n",
    "        gain = 1.\n",
    "        if re.match(r'.*decoder.*',n): \n",
    "            gain = (9*H.trf_dec)**(-1./4.)\n",
    "            if re.match(f'.*in_proj_weight$',n): gain *= (2**0.5)\n",
    "        elif re.match(r'.*encoder.*',n): \n",
    "            gain = 0.67*(H.trf_enc**(-1./4.))\n",
    "            if re.match(f'.*in_proj_weight$',n): gain *= (2**0.5)\n",
    "        if re.match(r'^embeds|^tagembeds', n): \n",
    "            trunc_normal_(p.data,std=(4.5*(H.trf_enc+H.trf_dec))**(-1./4.)*H.trf_dim**(-0.5))\n",
    "        else:                                  \n",
    "            nn.init.xavier_normal_(p,gain=gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "if H.tfixup:\n",
    "    class MyModelPatcher(ModelPatcher):\n",
    "        def new_child_module(self, child_module_name, child_module, patch_info): return nn.Identity()\n",
    "    mp = MyModelPatcher()\n",
    "    mp.add_pattern(r\".*norm\\d?.*\",{})\n",
    "    mp.patch_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TutorNet(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Embedding(3, 1, max_norm=1.0)\n",
       "      (1): Linear(in_features=1, out_features=512, bias=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Embedding(5, 3, max_norm=1.0)\n",
       "      (1): Linear(in_features=3, out_features=512, bias=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Embedding(9767, 274, max_norm=1.0)\n",
       "      (1): Linear(in_features=274, out_features=512, bias=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Embedding(6, 4, max_norm=1.0)\n",
       "      (1): Linear(in_features=4, out_features=512, bias=True)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Embedding(420, 47, max_norm=1.0)\n",
       "      (1): Linear(in_features=47, out_features=512, bias=True)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Embedding(9, 5, max_norm=1.0)\n",
       "      (1): Linear(in_features=5, out_features=512, bias=True)\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Embedding(4, 3, max_norm=1.0)\n",
       "      (1): Linear(in_features=3, out_features=512, bias=True)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Embedding(13525, 329, max_norm=1.0)\n",
       "      (1): Linear(in_features=329, out_features=512, bias=True)\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): Embedding(10002, 278, max_norm=1.0)\n",
       "      (1): Linear(in_features=278, out_features=512, bias=True)\n",
       "    )\n",
       "    (9): Sequential(\n",
       "      (0): Embedding(6, 4, max_norm=1.0)\n",
       "      (1): Linear(in_features=4, out_features=512, bias=True)\n",
       "    )\n",
       "    (10): Sequential(\n",
       "      (0): Embedding(7, 4, max_norm=1.0)\n",
       "      (1): Linear(in_features=4, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (tagembeds): EmbeddingBag(190, 512, max_norm=1.0, mode=sum)\n",
       "  (conts): Linear(in_features=23, out_features=512, bias=True)\n",
       "  (trafo): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "          (norm3): Identity()\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "          (norm3): Identity()\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "          (norm3): Identity()\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "          (norm3): Identity()\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): Identity()\n",
       "    )\n",
       "  )\n",
       "  (mlp): Linear(in_features=512, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#noexport\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dls.cuda()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delegates(Lamb)\n",
    "def ranger_lamb(p, lr, mom=0.95, wd=0.01, eps=1e-6, **kwargs):\n",
    "    #if(rank_distrib()==0): import pdb; pdb.set_trace()\n",
    "    if(isinstance(p[0],dict)): p = p[0]['params']\n",
    "    return Lookahead(Lamb(p, lr=lr, mom=mom, wd=wd, eps=eps, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_distrib()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noexport\n",
    "#!conda install -y -c conda-forge mpi4py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def deep_speed_opt_func(params,lr=0.01):\n",
    "    opt=globals()[H.opt](params,lr=lr,**H.opt_kwargs)\n",
    "    engine=DeepSpeedEngine(args,model,config=config,optimizer=opt)\n",
    "    return engine.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#H.zero_stage=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_func=partial(globals()[H.opt],**H.opt_kwargs)\n",
    "if(H.zero_stage>0):\n",
    "    #opt_func=partial(OptimWrapper,opt=opt_func,convert_groups=False)\n",
    "    opt_func = partial(OptimWrapper,opt=ZeroRedundancyOptimizer,convert_groups=False,optimizer_class=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(\n",
    "    dls,\n",
    "    model,\n",
    "    loss_func=ua_loss_func,\n",
    "    opt_func=opt_func,\n",
    "    moms = H.moms,\n",
    "    metrics=[\n",
    "        LBMetric(loss_func, 'acc_valid_loss'),\n",
    "        LBMetric(roc_auc, 'acc_roc_auc'),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_fp16 = getattr(learn,H.fp16,None)\n",
    "if f_fp16: f_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank0_only(func, *args, **kwargs):\n",
    "    \"Execute `func` in the Rank-0 process first, then in other ranks in parallel.\"\n",
    "    if args or kwargs: func = partial(func, *args, **kwargs)\n",
    "    dummy_l = Learner(DataLoaders(device='cpu'), nn.Linear(1,1), loss_func=lambda: 0)\n",
    "    res = None\n",
    "    with dummy_l.distrib_ctx():\n",
    "        if not rank_distrib(): res = func()\n",
    "        distrib_barrier()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def load(learn:Learner,fn,with_opt=False):\n",
    "    def __inner(learn:Learner,fn,with_opt=False):\n",
    "        m_dict = torch.load(f\"{(Path(learn.model_dir) / fn)}.pth\")#['model']\n",
    "        ks = []\n",
    "        for attempts in range(2):\n",
    "            try:\n",
    "                res = learn.model.load_state_dict(m_dict,strict=False)\n",
    "                print(f\"Loaded {fn} ignoring: {' '.join(ks)} and {res}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                for k in [m[1] for m in [re.match(r\"^.*mismatch for ([\\w\\.]+):\",l) for l in str(e).split(\"\\n\")] if m is not None]:\n",
    "                    m_dict.pop(k,None)\n",
    "                    ks.append(k)\n",
    "        return learn\n",
    "    return rank0_only(__inner, learn, fn, with_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if H.load:\n",
    "    learn.load(H.load, with_opt=False)\n",
    "    print(f\"Loaded: {H.load}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if H.clip: \n",
    "    learn.add_cb(GradientClipping(H.clip))\n",
    "    print('clip on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if H.local_rank is not None: \n",
    "    learn.to_distributed(H.local_rank)\n",
    "    print('local_rank on')\n",
    "if H.mixup: \n",
    "    learn.add_cb(MyMixUp(0.5))\n",
    "    print('mixup on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if H.validate:\n",
    "    res = learn.validate()\n",
    "    print(f\"CV: {res[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noexport\n",
    "learn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noexport\n",
    "#learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noexport\n",
    "#learn.recorder.plot_lr_find(skip_end=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#short_cb = learn.add_cb(ShortEpochCallback(pct=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.add_cb(SaveModelCallback(monitor='acc_roc_auc', comp=np.greater, fname=f'best{H.model}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(H.fit, H.epochs, H.lr, H.fit_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noexport\n",
    "syn_learn = synth_learner()\n",
    "getattr(syn_learn,H.fit)(H.epochs, H.lr,**H.fit_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noexport\n",
    "syn_learn.recorder.plot_sched()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.fit_flat_cos(H.epochs, H.lr,**H.fit_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pdb;pdb.set_trace()\n",
    "print(f\"Fitting {H.local_rank}\")\n",
    "getattr(learn,H.fit)(H.epochs, H.lr,**H.fit_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noexport\n",
    "learn.recorder.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noexport\n",
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
